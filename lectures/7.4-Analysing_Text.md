Theme: casa notes
Palette: Purple
Size: Wide
Title: File Formats
Author: Jon Reades

---
Layout: Title
# Analysing Text

---
### One-Hot Encoding

May already be familiar with concept as 'dummy variables' in economics/regression:

| Document      | UK   | Top  | Pop  | Coronavirus |
| ------------- | ---- | ---- | ---- | ----------- |
| News          | 1    | 1    | 0    | 1           |
| Culture       | 0    | 1    | 1    | 0           |
| Politics      | 1    | 0    | 0    | 1           |
| Entertainment | 1    | 1    | 1    | 1           |

^ Certainly One-Hot encoders are rarely, if ever, used this way, but for keyword detection this might be appropriate: i.e. this word was used in this document!

^ Only difference is One Hot == $$n$$ variables, Dummy == $$n-1$$.

^ Definitely some 'gotchas' in deployment: one-hot models shouldn't have an intercept unless you apply a 'ridge shrinkage penalty'. Standardisation affects whether or not an intercept is needed.

---
### The 'Bag of Words'

Could simply be seen as an extension of binarised approach on preceding slide:

| Document      | UK   | Top  | Pop  | Coronavirus |
| ------------- | ---- | ---- | ---- | ----------- |
| News          | 4    | 2    | 0    | 6           |
| Culture       | 0    | 4    | 7    | 0           |
| Politics      | 3    | 0    | 0    | 3           |
| Entertainment | 3    | 4    | 8    | 1           |

---
### BoW in Practice

Enter, stage left, [scikit-learn](https://scikit-learn.org/stable/):

```python
from sklearn.feature_extraction.text import CountVectorizer
vectorizer = CountVectorizer()

vectorizer.fit(texts)
vectors = vectorizer.transform(texts)

# Same thing:
# vectors = vectorizer.fit_transform(texts)

print(f'Vocabulary: {vectorizer.vocabulary_}')
print(f'Full vector: {vectors.toarray()}')
```

---
### TF/IDF

Builds on Count Vectorisation by normalising the document frequency measure by the overall corpus frequency. Common words receive a large penalty:

$$
W(t,d) = TF(t,d) / log(N/DF_{t})
$$

For example: if the term 'cat' appears 3 times in a document of 100 words then $TF(t,d)=3/100$. If there are 10,000 documents and cat appears in 1,000 documents then $N/DF_{t}=10000/1000$ and $log(10)=1$, so IDF=1 and TF/IDF=0.03.

---

### TF/IDF in Practice

```python
from sklearn.feature_extraction.text import TfidfVectorizer
vectorizer = TfidfVectorizer()

vectorizer.fit(texts)
vectors = vectorizer.transform(texts)

# Same thing:
# vectors=vectorizer.fit_transform(texts)

print(f'Vocabulary: {vectorizer.vocabulary_}')
print(f'Full vector: {vectors.toarray()}')
```

^ What do you notice about how this code differs from the CountVectorizer?

---

### Term Co-Occurence Matrix (TCM)

Three input texts:

- the cat sat on the mat
- the cat sat on the fluffy mat
- the fluffy ginger cat sat on the mat

|        | fluffy |  mat | ginger |  sat |   on |  cat |  the |
| :----- | -----: | ---: | -----: | ---: | ---: | ---: | ---: |
| fluffy |        |    1 |      1 |      |  0.5 |  0.5 |  2.0 |
| mat    |        |      |        |      |  0.5 |      |  1.5 |
| ginger |        |      |        |  0.5 |  0.5 |  1.0 |  1.5 |
| sat    |        |      |        |      |  3.0 |  3.0 |  2.5 |
| on     |        |      |        |      |      |  1.5 |  3.0 |
| cat    |        |      |        |      |      |      |  2.0 |
| the    |        |      |        |      |      |      |      |

---
### Text2Vec

Typically some kind of 2 or 3-layer neural network that 'learns' (using as big a training data set as possible) how to embed the TCM into a lower-dimension representation. 

Conceptual similarities to PCA in terms of what we're trying to achieve, but the *process* is utterly different.

Many different approaches, but [GloVe](https://nlp.stanford.edu/projects/glove/) (Stanford), [word2vec](https://code.google.com/archive/p/word2vec/) (Google), [fastText](https://fasttext.cc/docs/en/english-vectors.html) (Facebook), and [ELMo](https://allennlp.org/elmo) (Allen) or [BERT](https://github.com/google-research/bert) (Google) are probably the best-known.

---
### Sentiment Analysis

Requires us to deal in great detail with bi- and tri-grams because *negation* and *sarcasm* are hard. Also tends to require training/labelled data.

<div align="center">
<a href="https://medium.com/@tomyuz/a-sentiment-analysis-approach-to-predicting-stock-returns-d5ca8b75a42">
<img src="https://raw.githubusercontent.com/jreades/i2p/master/lectures/img/Sentiment_Analysis.png" width="500" />
</a>
</div>

---
### Clustering

| Cluster | Geography | Earth Science | History | Computer Science |  Total |
| :------ | --------: | ------------: | ------: | ---------------: | -----: |
| 1       |       126 |           310 |     104 |           11,018 | 11,558 |
| 2       |       252 |        10,673 |     528 |              126 | 11,579 |
| 3       |       803 |           485 |   6,730 |              135 |  8,153 |
| 4       |       100 |           109 |   6,389 |               28 |  6,626 |
| Total   |     1,281 |        11,577 |  13,751 |           11,307 | 37,916 |

---
### Topic Modelling

Learning associations of words (or images or many other things) to hidden 'topics' that generate them:

![](img/LDA.png)

---

### Word Clouds

![](img/Cluster_Clouds.png)

---

## Resources

- [One-Hot vs Dummy Encoding](https://stats.stackexchange.com/questions/224051/one-hot-vs-dummy-encoding-in-scikit-learn)
- [Categorical encoding using Label-Encoding and One-Hot-Encoder](https://towardsdatascience.com/categorical-encoding-using-label-encoding-and-one-hot-encoder-911ef77fb5bd)
- [Count Vectorization with scikit-learn](https://towardsdatascience.com/natural-language-processing-count-vectorization-with-scikit-learn-e7804269bb5e)
- [TFIDF.com](http://www.tfidf.com/)
- [The TF*IDF Algorithm Explained](https://www.onely.com/blog/what-is-tf-idf/)
- [How to Use TfidfTransformer and TfidfVectorizer](https://kavita-ganesan.com/tfidftransformer-tfidfvectorizer-usage-differences/#.X7gXhhP7Tlw)
- [SciKit Learn Feature Extraction](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.feature_extraction)
- [Your Guide to LDA](https://medium.com/@lettier/how-does-lda-work-ill-explain-using-emoji-108abf40fa7d)
- [Machine Learning — Latent Dirichlet Allocation LDA](https://jonathan-hui.medium.com/machine-learning-latent-dirichlet-allocation-lda-1d9d148f13a4)
- [A Beginner’s Guide to Latent Dirichlet Allocation(LDA)](https://towardsdatascience.com/latent-dirichlet-allocation-lda-9d1cd064ffa2)
- [Analyzing Documents with TF-IDF](https://programminghistorian.org/en/lessons/analyzing-documents-with-tfidf)

Basically any of the lessons on [The Programming Historian](https://programminghistorian.org/en/lessons/).

---
### More Resources
- [Introduction to Word Embeddings](https://towardsdatascience.com/introduction-to-word-embeddings-4cf857b12edc)
- [The Current Best of Universal Word Embeddings and Sentence Embeddings](https://medium.com/huggingface/universal-word-sentence-embeddings-ce48ddc8fc3a)
- [Using GloVe Embeddings](http://text2vec.org/glove.html)
- [Working with Facebook's FastText Library](https://stackabuse.com/python-for-nlp-working-with-facebook-fasttext-library/)
- [Word2Vec and FastText Word Embedding with Gensim](https://towardsdatascience.com/word-embedding-with-word2vec-and-fasttext-a209c1d3e12c)
- [Sentence Embeddings. Fast, please!](https://towardsdatascience.com/fse-2b1ffa791cf9)
- [PlasticityAI Embedding Models](https://github.com/plasticityai/magnitude)
- [Clustering text documents using *k*-means](https://scikit-learn.org/stable/auto_examples/text/plot_document_clustering.html#sphx-glr-auto-examples-text-plot-document-clustering-py)
- [Topic extraction with Non-negative Matrix Factorization and LDA](https://scikit-learn.org/stable/auto_examples/applications/plot_topics_extraction_with_nmf_lda.html#sphx-glr-auto-examples-applications-plot-topics-extraction-with-nmf-lda-py)