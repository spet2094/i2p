Theme: casa notes
Palette: Purple
Size: Wide
Title: File Formats
Author: Jon Reades

---
Layout: Title
# Cleaning Text

---
Layout: SectionTitle
## Vectorisation & Parallelisation

---
### Say What?

> Bye, bye, for loop!

^ Another great example of something where what makes sense to a human is not always the fastest way to *get things done*.

---
### Do More with Each Tick-Tock

#### **Implicit**

- Library/Package implements weak form of vectorisation or parallelisation, but some libraries do more.

#### **Explicit**

- You must request it because it requires hardware or other support and it is highly optimsed.

#### **Massive (a.k.a. Distributed)**

- Multiple separate machines acting as one.
- Multiple GPUs acting as one.

^ Conceptually, these get challenging if you can't clearly separate/parallelise tasks.

---
### Pandas.apply() vs. Numpy

Numpy is fully vectorised and will almost *always* out-perform operations like Pandas `apply`, but both are massive improvements on for loops:

- Execute row-wise and column-wise operations.
- Apply any arbitrary function to individual elements or whole axes.
- Can make use of `lambda` functions too for 'one off' operations.

```python
import numpy as np
df.apply(np.sqrt) # Square root of all values
df.apply(np.sum, axis=0) # Sum by row
```

---
### Lambda Functions

Functional equivalent of list comprehensions: 1-line, anonymous functions.

For example:

```python
>>> x = lambda a : a + 10
>>> print(x(5))
15
```

Or:

```python
>>> full_name = lambda first, last: f'Full name: {first.title()} {last.title()}'
>>> full_name('guido', 'van rossum')
'Guido Van Rossum'
```

These are very useful with pandas.

---
Layout: SectionTitle
## Dealing with Structured Text

---
### Beautiful Soup & Selenium

Two stages to acquiring web-based documents:

1. Accessing the document: `urllib` can deal with many issues (even authentication), but *not* with dynamic web pages (which are increasingly common); for that, you need [Selenium](https://selenium-python.readthedocs.io/) (library + driver).
2. Processing the document: simple data can be extracted from web pages with RegularExpressions, but *not* with complex (esp. dynamic) content; for that, you need [BeautifulSoup4](https://www.crummy.com/software/BeautifulSoup/bs4/doc/).

These interact with wider issues of Fair Use (e.g. rate limits and licenses); processing pipelines (e.g. saving WARCs or just the text file, multiple stages, etc.); and other practical constraints.

---
### Regular Expressions / Breaks

Need to look at how the data is organised:

- For very large corpora, you might want one document at a time (batch).
- For very large files, you might want one line at a time (streaming).
- For large files in large corpora, you might want more than one machine.

^ See the [OpenVirus Project](https://blogs.bl.uk/digital-scholarship/2020/05/searching-etheses-for-the-openvirus-project.html).

---
Layout: SectionTitle
## Managing Vocabularies

^ A lot of what you'll see here are, at best, heuristics with enormous variation in practice.

---
### Starting Points

These strategies can be sued singly or all-together:

- Stopwords
- Case
- Accent-stripping
- Punctuation
- Numbers

Sample stopwords:

```python
{'further', 'her', 'their', 'we', 'just', 'why', 'or', 'each', 's', "it's", 'ma', 'below', 'am', 'more', "couldn't", "should've", 'was', "mightn't", 'weren', 'ourselves', 'have', 'if', 'then', 'from', ...}
```

But these are just a *starting* point!

^ What's the semantic difference between 1,000,000 and 999,999?

---

### Distributional Pruning

We can prune from both ends of the distribution:

- Overly rare words: what does a word used in *one* document help us to do?
- Overly common ones: what does a word used in *every* document help us to do?

^ Again, no hard-and-fast rules: can be done on raw counts, percentage of all documents, etc. Choices will, realistically, depend on the nature of the data.

---
Layout: SectionTitle
## Stemming & Lemmatisation
---
### Why Stem or Lemmatise?

Reduce the breadth of human expression:

- Porter & Snowball Stemming: rules-based truncation to a stem (can be augmented by language awareness).
- Lemmatisation: dictionary-based 'deduplication' to a lemma (can be augmented by POS-tagging).

Compare:

| Source     | Porter  | Snowball | Lemmatisation |
| ---------- | ------- | -------- | ------------- |
| monkeys    | monkey  | monkey   | monkey        |
| cities     | citi    | citi     | city          |
| complexity | complex | complex  | complexity    |
| Reades     | read    | read     | Reades        |



---

## Resources

- [Vectorisation in Python](https://towardsdatascience.com/python-vectorization-5b882eeef658)
- [Lambda Functions](https://www.w3schools.com/python/python_lambda.asp)
- [Real Python Lambda Functions](https://realpython.com/python-lambda/)
- [Stemming words with NLTK](https://pythonprogramming.net/stemming-nltk-tutorial/)
- [Stemming and Lemmatisation in Python](https://www.datacamp.com/community/tutorials/stemming-lemmatization-python)
- [KD Nuggets: A Practitioner's Guide to NLP](https://www.kdnuggets.com/2018/08/practitioners-guide-processing-understanding-text-2.html)
- [KD Nuggets: Linguistic Fundamentals for Natural Language Processing: 100 Essentials from Semantics and Pragmatics](https://www.kdnuggets.com/2020/08/linguistic-fundamentals-natural-language-processing.html)
- [Roadmap to Natural Language Processing (NLP)](https://www.kdnuggets.com/2020/10/roadmap-natural-language-processing-nlp.html)
- 