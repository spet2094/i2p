{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"float:left\">\n",
    "    <h1 style=\"width:450px\">Practical 9: Dimensions in Data</h1>\n",
    "    <h2 style=\"width:450px\">Transformation &amp; Dimensionality Reduction</h2>\n",
    "</div>\n",
    "<div style=\"float:right\"><img width=\"100\" src=\"https://github.com/jreades/i2p/raw/master/img/casa_logo.jpg\" /></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this session the focus is on MSOA-level Census data from 2011. Although it's not ideal to link 2011 data to 2020 data, we: a) have no other choice; and b) could actually do a bit of thinking about whether the situation in 2011 in some way helps us to predict the situation now..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preamble\n",
    "\n",
    "Let's start with the usual bits of code to ensure plotting works, to import packages and load the data into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import seaborn as sns\n",
    "\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display_markdown\n",
    "\n",
    "# This has been expanded to try to print out Pandas Series\n",
    "# objects a little more intelligently as a Markdown table!\n",
    "# We use f-strings everywhere because it ensures that we\n",
    "# are working with string outputs.\n",
    "def markdown_body(txt):\n",
    "    if type(txt)==str:\n",
    "        return \"> \" + txt\n",
    "    elif type(txt)==pd.core.series.Series:\n",
    "        out = '| Index | Value |\\n| :----- | -----: |\\n'\n",
    "        for i in range(0,txt.shape[0]):\n",
    "            out += f\"| {txt.index[i]} | \"\n",
    "            if type(txt.iloc[i])==float or type(txt.iloc[i])==int or type(txt.iloc[i])==np.float64:\n",
    "                out += f\"{txt.iloc[i]:0.2f}\"\n",
    "            else:\n",
    "                out += f\"{txt.iloc[i]}\"\n",
    "            out += ' |\\n'\n",
    "        return out\n",
    "    else:\n",
    "        print(type(txt))\n",
    "        return \"> \" + txt\n",
    "\n",
    "# Notice how this has changed slightly to \n",
    "# call a function in the f-string instead\n",
    "# of simply outputting the value. So we have\n",
    "# a function calling a function!\n",
    "def as_markdown(head='', body='Some body text'):\n",
    "    if head != '':\n",
    "        display_markdown(f\"##### {head}\\n\\n{markdown_body(body)}\\n\", raw=True)\n",
    "    else:\n",
    "        display_markdown(f\"{markdown_body(body)}\\n\", raw=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading MSOA Census Data\n",
    "\n",
    "Now we're going to add in _one_ more data set: London's MSOA 'Atlas' from the [London Data Store](https://data.london.gov.uk/dataset)! I would _strongly_ suggest that you have a look around the Data Store as you develop your thinking for the final assessment -- you will likely find useful additional data there!\n",
    "\n",
    "Once you see how we deal with this MSOA Atlas data you will be in a position to look at the socio-economic **context** of Airbnb listings at the MSOA level, or with any other similar data set. If you're feeling particularly ambitious you can actually do this _same_ work at the LSOA scale using the [LSOA Atlas](https://data.london.gov.uk/dataset/lsoa-atlas) and LSOA boundaries... the process should be the same, though you will have smaller samples in each LSOA than you do in the MSOAs and calculations will take a bit longer to complete.\n",
    "\n",
    "There is a CSV file for the MSOA Atlast that would be easier to work with; however, the Excel file is useful for demonstrating how to work with multi-level indexes (an extension of last week's work). Notice that below we do two new things when reading the XLS file:\n",
    "\n",
    "1. We have to specify a sheet name because the file contains multiple sheets.\n",
    "2. We have to specify a header _and_ we actually have to specify three of them which generates a multi-level index (row 0 is the top-level, row 1 is the second-level, etc.).\n",
    "\n",
    "You might like to load a copy of the Excel file into Excel so that you can see how the next bit works. You can find the MSOA Atlas [here](https://data.london.gov.uk/dataset/msoa-atlas)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "msoa_atlas = pd.read_excel(\n",
    "    'https://data.london.gov.uk/download/msoa-atlas/39fdd8eb-e977-4d32-85a4-f65b92f29dcb/msoa-data.xls', \n",
    "    sheet_name='iadatasheet1', # Which sheet is the data in?\n",
    "    header=[0,1,2])            # Where are the column names... there's three of them!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the format of the output and notice that all of the empty cells in the Excel sheet have come through as `Unnamed: <col_no>_level_<level_no>`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0_level_0</th>\n",
       "      <th>Unnamed: 1_level_0</th>\n",
       "      <th colspan=\"7\" halign=\"left\">Age Structure (2011 Census)</th>\n",
       "      <th>Mid-year Estimate totals</th>\n",
       "      <th>...</th>\n",
       "      <th colspan=\"10\" halign=\"left\">Road Casualties</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0_level_1</th>\n",
       "      <th>Unnamed: 1_level_1</th>\n",
       "      <th>All Ages</th>\n",
       "      <th>0-15</th>\n",
       "      <th>16-29</th>\n",
       "      <th>30-44</th>\n",
       "      <th>45-64</th>\n",
       "      <th>65+</th>\n",
       "      <th>Working-age</th>\n",
       "      <th>All Ages</th>\n",
       "      <th>...</th>\n",
       "      <th colspan=\"2\" halign=\"left\">2010</th>\n",
       "      <th colspan=\"4\" halign=\"left\">2011</th>\n",
       "      <th colspan=\"4\" halign=\"left\">2012</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>MSOA Code</th>\n",
       "      <th>MSOA Name</th>\n",
       "      <th>Unnamed: 2_level_2</th>\n",
       "      <th>Unnamed: 3_level_2</th>\n",
       "      <th>Unnamed: 4_level_2</th>\n",
       "      <th>Unnamed: 5_level_2</th>\n",
       "      <th>Unnamed: 6_level_2</th>\n",
       "      <th>Unnamed: 7_level_2</th>\n",
       "      <th>Unnamed: 8_level_2</th>\n",
       "      <th>2002</th>\n",
       "      <th>...</th>\n",
       "      <th>Slight</th>\n",
       "      <th>2010 Total</th>\n",
       "      <th>Fatal</th>\n",
       "      <th>Serious</th>\n",
       "      <th>Slight</th>\n",
       "      <th>2011 Total</th>\n",
       "      <th>Fatal</th>\n",
       "      <th>Serious</th>\n",
       "      <th>Slight</th>\n",
       "      <th>2012 Total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>E02000001</td>\n",
       "      <td>City of London 001</td>\n",
       "      <td>7375.0</td>\n",
       "      <td>620.0</td>\n",
       "      <td>1665.0</td>\n",
       "      <td>2045.0</td>\n",
       "      <td>2010.0</td>\n",
       "      <td>1035.0</td>\n",
       "      <td>5720.0</td>\n",
       "      <td>7280.0</td>\n",
       "      <td>...</td>\n",
       "      <td>334.0</td>\n",
       "      <td>374.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>359.0</td>\n",
       "      <td>405.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>361.0</td>\n",
       "      <td>414.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>E02000002</td>\n",
       "      <td>Barking and Dagenham 001</td>\n",
       "      <td>6775.0</td>\n",
       "      <td>1751.0</td>\n",
       "      <td>1277.0</td>\n",
       "      <td>1388.0</td>\n",
       "      <td>1258.0</td>\n",
       "      <td>1101.0</td>\n",
       "      <td>3923.0</td>\n",
       "      <td>6333.0</td>\n",
       "      <td>...</td>\n",
       "      <td>18.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>E02000003</td>\n",
       "      <td>Barking and Dagenham 002</td>\n",
       "      <td>10045.0</td>\n",
       "      <td>2247.0</td>\n",
       "      <td>1959.0</td>\n",
       "      <td>2300.0</td>\n",
       "      <td>2259.0</td>\n",
       "      <td>1280.0</td>\n",
       "      <td>6518.0</td>\n",
       "      <td>9236.0</td>\n",
       "      <td>...</td>\n",
       "      <td>34.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 207 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Unnamed: 0_level_0        Unnamed: 1_level_0 Age Structure (2011 Census)  \\\n",
       "  Unnamed: 0_level_1        Unnamed: 1_level_1                    All Ages   \n",
       "           MSOA Code                 MSOA Name          Unnamed: 2_level_2   \n",
       "0          E02000001        City of London 001                      7375.0   \n",
       "1          E02000002  Barking and Dagenham 001                      6775.0   \n",
       "2          E02000003  Barking and Dagenham 002                     10045.0   \n",
       "\n",
       "                                                                               \\\n",
       "                0-15              16-29              30-44              45-64   \n",
       "  Unnamed: 3_level_2 Unnamed: 4_level_2 Unnamed: 5_level_2 Unnamed: 6_level_2   \n",
       "0              620.0             1665.0             2045.0             2010.0   \n",
       "1             1751.0             1277.0             1388.0             1258.0   \n",
       "2             2247.0             1959.0             2300.0             2259.0   \n",
       "\n",
       "                                        Mid-year Estimate totals  ...  \\\n",
       "                 65+        Working-age                 All Ages  ...   \n",
       "  Unnamed: 7_level_2 Unnamed: 8_level_2                     2002  ...   \n",
       "0             1035.0             5720.0                   7280.0  ...   \n",
       "1             1101.0             3923.0                   6333.0  ...   \n",
       "2             1280.0             6518.0                   9236.0  ...   \n",
       "\n",
       "  Road Casualties                                                           \\\n",
       "             2010             2011                            2012           \n",
       "           Slight 2010 Total Fatal Serious Slight 2011 Total Fatal Serious   \n",
       "0           334.0      374.0   0.0    46.0  359.0      405.0   2.0    51.0   \n",
       "1            18.0       18.0   0.0     2.0   16.0       18.0   0.0     1.0   \n",
       "2            34.0       37.0   1.0     4.0   40.0       45.0   0.0     3.0   \n",
       "\n",
       "                     \n",
       "                     \n",
       "  Slight 2012 Total  \n",
       "0  361.0      414.0  \n",
       "1   15.0       16.0  \n",
       "2   47.0       50.0  \n",
       "\n",
       "[3 rows x 207 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msoa_atlas.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the MSOA Atlas data frame is: 984 x 207\n"
     ]
    }
   ],
   "source": [
    "print(f\"Shape of the MSOA Atlas data frame is: {msoa_atlas.shape[0]} x {msoa_atlas.shape[1]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should get: `Shape of the MSOA Atlas data frame is: 984 x 207`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And this should look a _bit_ familiar from last week's work with _grouped data frames_!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Unnamed: 0_level_0', 'Unnamed: 1_level_0',\n",
      "       'Age Structure (2011 Census)', 'Age Structure (2011 Census)',\n",
      "       'Age Structure (2011 Census)', 'Age Structure (2011 Census)',\n",
      "       'Age Structure (2011 Census)', 'Age Structure (2011 Census)',\n",
      "       'Age Structure (2011 Census)', 'Mid-year Estimate totals',\n",
      "       ...\n",
      "       'Road Casualties', 'Road Casualties', 'Road Casualties',\n",
      "       'Road Casualties', 'Road Casualties', 'Road Casualties',\n",
      "       'Road Casualties', 'Road Casualties', 'Road Casualties',\n",
      "       'Road Casualties'],\n",
      "      dtype='object', length=207)\n"
     ]
    }
   ],
   "source": [
    "print(msoa_atlas.columns.get_level_values(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The _new_ thing to note here is that if we drop a level 0 index all then of the columns that it supports (levels 1 and 2) are _also_ dropped: so when we drop `Mid-year Estimate totals` from level 0 then all 11 of the 'Mid-year Estimate totals (2002...2012)' columns are dropped in one go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"11\" halign=\"left\">Mid-year Estimate totals</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"11\" halign=\"left\">All Ages</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>2002</th>\n",
       "      <th>2003</th>\n",
       "      <th>2004</th>\n",
       "      <th>2005</th>\n",
       "      <th>2006</th>\n",
       "      <th>2007</th>\n",
       "      <th>2008</th>\n",
       "      <th>2009</th>\n",
       "      <th>2010</th>\n",
       "      <th>2011</th>\n",
       "      <th>2012</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7280.0</td>\n",
       "      <td>7115.0</td>\n",
       "      <td>7118.0</td>\n",
       "      <td>7131.0</td>\n",
       "      <td>7254.0</td>\n",
       "      <td>7607.0</td>\n",
       "      <td>7429.0</td>\n",
       "      <td>7472.0</td>\n",
       "      <td>7338.0</td>\n",
       "      <td>7412.0</td>\n",
       "      <td>7604.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6333.0</td>\n",
       "      <td>6312.0</td>\n",
       "      <td>6329.0</td>\n",
       "      <td>6341.0</td>\n",
       "      <td>6330.0</td>\n",
       "      <td>6323.0</td>\n",
       "      <td>6369.0</td>\n",
       "      <td>6570.0</td>\n",
       "      <td>6636.0</td>\n",
       "      <td>6783.0</td>\n",
       "      <td>6853.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9236.0</td>\n",
       "      <td>9252.0</td>\n",
       "      <td>9155.0</td>\n",
       "      <td>9072.0</td>\n",
       "      <td>9144.0</td>\n",
       "      <td>9227.0</td>\n",
       "      <td>9564.0</td>\n",
       "      <td>9914.0</td>\n",
       "      <td>10042.0</td>\n",
       "      <td>10088.0</td>\n",
       "      <td>10218.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Mid-year Estimate totals                                                  \\\n",
       "                  All Ages                                                   \n",
       "                      2002    2003    2004    2005    2006    2007    2008   \n",
       "0                   7280.0  7115.0  7118.0  7131.0  7254.0  7607.0  7429.0   \n",
       "1                   6333.0  6312.0  6329.0  6341.0  6330.0  6323.0  6369.0   \n",
       "2                   9236.0  9252.0  9155.0  9072.0  9144.0  9227.0  9564.0   \n",
       "\n",
       "                                      \n",
       "                                      \n",
       "     2009     2010     2011     2012  \n",
       "0  7472.0   7338.0   7412.0   7604.0  \n",
       "1  6570.0   6636.0   6783.0   6853.0  \n",
       "2  9914.0  10042.0  10088.0  10218.0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msoa_atlas[['Mid-year Estimate totals']].head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the MSOA Atlas data frame is now: 984 x 111\n"
     ]
    }
   ],
   "source": [
    "to_drop = ['Mid-year Estimate totals','Mid-year Estimates 2012, by age','Religion (2011)',\n",
    "           'Land Area','Lone Parents (2011 Census)','Central Heating (2011 Census)','Health (2011 Census)',\n",
    "           'Low Birth Weight Births (2007-2011)','Obesity','Incidence of Cancer','Life Expectancy',\n",
    "           'Road Casualties']\n",
    "msoa_atlas.drop(to_drop, axis=1, level=0, inplace=True)\n",
    "print(f\"Shape of the MSOA Atlas data frame is now: {msoa_atlas.shape[0]} x {msoa_atlas.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This should drop you down to `984 x 111`. Notice below that the multi-level _index_ has not changed but the multi-level _values_ remaining have!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Adults in Employment (2011 Census)', 'Age Structure (2011 Census)',\n",
      "       'Car or van availability (2011 Census)',\n",
      "       'Central Heating (2011 Census)', 'Country of Birth (2011)',\n",
      "       'Dwelling type (2011)', 'Economic Activity (2011 Census)',\n",
      "       'Ethnic Group (2011 Census)', 'Health (2011 Census)', 'House Prices',\n",
      "       'Household Composition (2011)', 'Household Income Estimates (2011/12)',\n",
      "       'Household Language (2011)', 'Households (2011)', 'Incidence of Cancer',\n",
      "       'Income Deprivation (2010)', 'Land Area', 'Life Expectancy',\n",
      "       'Lone Parents (2011 Census)', 'Low Birth Weight Births (2007-2011)',\n",
      "       'Mid-year Estimate totals', 'Mid-year Estimates 2012, by age',\n",
      "       'Obesity', 'Population Density', 'Qualifications (2011 Census)',\n",
      "       'Religion (2011)', 'Road Casualties', 'Tenure (2011)',\n",
      "       'Unnamed: 0_level_0', 'Unnamed: 1_level_0'],\n",
      "      dtype='object')\n",
      "Index(['Unnamed: 0_level_0', 'Unnamed: 1_level_0',\n",
      "       'Age Structure (2011 Census)', 'Age Structure (2011 Census)',\n",
      "       'Age Structure (2011 Census)', 'Age Structure (2011 Census)',\n",
      "       'Age Structure (2011 Census)', 'Age Structure (2011 Census)',\n",
      "       'Age Structure (2011 Census)', 'Households (2011)',\n",
      "       ...\n",
      "       'Car or van availability (2011 Census)',\n",
      "       'Car or van availability (2011 Census)',\n",
      "       'Car or van availability (2011 Census)',\n",
      "       'Car or van availability (2011 Census)',\n",
      "       'Car or van availability (2011 Census)',\n",
      "       'Car or van availability (2011 Census)',\n",
      "       'Car or van availability (2011 Census)',\n",
      "       'Car or van availability (2011 Census)',\n",
      "       'Car or van availability (2011 Census)',\n",
      "       'Car or van availability (2011 Census)'],\n",
      "      dtype='object', length=111)\n"
     ]
    }
   ],
   "source": [
    "print(msoa_atlas.columns.levels[0]) # The categories\n",
    "print(msoa_atlas.columns.get_level_values(0)) # The actual values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 1.1: Selecting Columns using a List Comprehension\n",
    "\n",
    "Now we need to drop all of the percentages from the data set. These can be found at level 1, though they are specified in a number of different ways so you'll need to come up with a way to find them in the level 1 values using a list comprehension... \n",
    "\n",
    "I'd suggest looking for: \"(%)\", \"%\", and \"Percentages\". You may need to check both start and end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Percentages', 'Percentages', 'Percentages', 'Percentages', 'Percentages', 'White (%)', 'Mixed/multiple ethnic groups (%)', 'Asian/Asian British (%)', 'Black/African/Caribbean/Black British (%)', 'Other ethnic group (%)', 'BAME (%)', 'United Kingdom (%)', 'Not United Kingdom (%)', '% of people aged 16 and over in household have English as a main language', '% of households where no people in household have English as a main language', 'Owned: Owned outright (%)', 'Owned: Owned with a mortgage or loan (%)', 'Social rented (%)', 'Private rented (%)', 'Household spaces with at least one usual resident (%)', 'Household spaces with no usual residents (%)', 'Whole house or bungalow: Detached (%)', 'Whole house or bungalow: Semi-detached (%)', 'Whole house or bungalow: Terraced (including end-terrace) (%)', 'Flat, maisonette or apartment (%)', 'Economically active %', 'Economically inactive %', '% of households with no adults in employment: With dependent children', '% living in income deprived households reliant on means tested benefit', '% of people aged over 60 who live in pension credit households', 'No cars or vans in household (%)', '1 car or van in household (%)', '2 cars or vans in household (%)', '3 cars or vans in household (%)', '4 or more cars or vans in household (%)']\n"
     ]
    }
   ],
   "source": [
    "to_drop = [x for x in msoa_atlas.columns.get_level_values(1) if (\n",
    "    x.endswith(\"(%)\") or x.startswith(\"%\") or x.endswith(\"Percentages\") or x.endswith(\"%\"))]\n",
    "print(to_drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should get:\n",
    "```\n",
    "['Percentages', 'Percentages', 'Percentages', 'Percentages', 'Percentages', 'White (%)', 'Mixed/multiple ethnic groups (%)', 'Asian/Asian British (%)', 'Black/African/Caribbean/Black British (%)', 'Other ethnic group (%)', 'BAME (%)', 'United Kingdom (%)', 'Not United Kingdom (%)', '% of people aged 16 and over in household have English as a main language', '% of households where no people in household have English as a main language', 'Owned: Owned outright (%)', 'Owned: Owned with a mortgage or loan (%)', 'Social rented (%)', 'Private rented (%)', 'Household spaces with at least one usual resident (%)', 'Household spaces with no usual residents (%)', 'Whole house or bungalow: Detached (%)', 'Whole house or bungalow: Semi-detached (%)', 'Whole house or bungalow: Terraced (including end-terrace) (%)', 'Flat, maisonette or apartment (%)', 'Economically active %', 'Economically inactive %', '% of households with no adults in employment: With dependent children', '% living in income deprived households reliant on means tested benefit', '% of people aged over 60 who live in pension credit households', 'No cars or vans in household (%)', '1 car or van in household (%)', '2 cars or vans in household (%)', '3 cars or vans in household (%)', '4 or more cars or vans in household (%)']\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 1.2: Drop by Level\n",
    "\n",
    "You now need to drop these columns using the `level` keyword as part of your drop command. You have plenty of examples of how to drop values in place, but I'd suggest _first_ getting the command correct and allowing to it return a new data frame and _then_ adding the in place parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the MSOA Atlas data frame is now: 984 x 76\n"
     ]
    }
   ],
   "source": [
    "msoa_atlas.drop(to_drop, axis=1, level=1, inplace=True)\n",
    "print(f\"Shape of the MSOA Atlas data frame is now: {msoa_atlas.shape[0]} x {msoa_atlas.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data frame is now `984 x 76`. This is a _bit_ more manageable though still a _lot_ of data columns. Depending on what you decide to do for your final project you might want to revisit some of the columns that we dropped above... "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 1.3: Flattening the Index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Level 2 of the multi-index is mainly composed of 'Unnamed' values so we want to merge it with Level 1 to simplify our data frame, and then merge _that_ with level 0... Unlike a 'regular' data frame, asking for the column values on a data frame with a hierarhical index will give you an array of tuples: with one element for each level of the index for _each) column!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([('Unnamed: 0_level_0', 'Unnamed: 0_level_1', 'MSOA Code'),\n",
       "       ('Unnamed: 1_level_0', 'Unnamed: 1_level_1', 'MSOA Name'),\n",
       "       ('Age Structure (2011 Census)', 'All Ages', 'Unnamed: 2_level_2')],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msoa_atlas.columns.values[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['MSOA Code', 'MSOA Name', 'Age - All Ages', 'Age - 0-15', 'Age - 16-29', 'Age - 30-44', 'Age - 45-64', 'Age - 65+', 'Age - Working-age', 'Households - All Households', 'Composition - Couple hh with dependent children', 'Composition - Couple hh without dependent children', 'Composition - Lone parent hh', 'Composition - One person hh', 'Composition - Other hh Types', 'White', 'Mixed/multiple ethnic groups', 'Asian/Asian British', 'Black/African/Caribbean/Black British', 'Other ethnic group', 'BAME', 'Country of Birth - United Kingdom', 'Country of Birth - Not United Kingdom', 'Language - 1+ English as a main language', 'Language - None have English as main language', 'Tenure - Owned outright', 'Tenure - Owned with a mortgage or loan', 'Tenure - Social rented', 'Tenure - Private rented', 'Household spaces with at least one usual resident', 'Household spaces with no usual residents', 'Detached', 'Semi-detached', 'Terraced (including end-terrace)', 'Flat, maisonette or apartment', 'Population Density - Persons per hectare (2012)', 'Median - 2005', 'Median - 2006', 'Median - 2007', 'Median - 2008', 'Median - 2009', 'Median - 2010', 'Median - 2011', 'Median - 2012', 'Median - 2013 (p)', 'Sales - 2005', 'Sales - 2006', 'Sales - 2007', 'Sales - 2008', 'Sales - 2009', 'Sales - 2010', 'Sales - 2011', 'Sales - 2011.1', 'Sales - 2013(p)', 'Qualifications - No', 'Qualifications - Level 1', 'Qualifications - Level 2', 'Qualifications - Apprenticeship', 'Qualifications - Level 3', 'Qualifications - Level 4 and above', 'Qualifications - Other', 'Qualifications - Schoolchildren and full-time students: Age 18 and over', 'Economic Activity - Economically active: Total', 'Economic Activity - Economically active: Unemployed', 'Economic Activity - Economically inactive: Total', 'Economic Activity - Unemployment Rate', 'Adults in Employment - No adults in employment in hh: With dependent children', 'Total Mean hh Income', 'Total Median hh Income', 'Vehicles - No cars or vans in hh', 'Vehicles - 1 car or van in hh', 'Vehicles - 2 cars or vans in hh', 'Vehicles - 3 cars or vans in hh', 'Vehicles - 4 or more cars or vans in hh', 'Vehicles - Sum of all cars or vans in the area', 'Vehicles - Cars per hh']\n"
     ]
    }
   ],
   "source": [
    "new_cols = []\n",
    "for c in msoa_atlas.columns.values:\n",
    "    \n",
    "    #print(f\"Column label: {c}\")\n",
    "    l1 = f\"{c[0]}\"\n",
    "    l2 = f\"{c[1]}\"\n",
    "    l3 = f\"{c[2]}\"\n",
    "    \n",
    "    # The new column label\n",
    "    clabel = ''\n",
    "    \n",
    "    # Assemble new label from the levels\n",
    "    if not l1.startswith(\"Unnamed\"):\n",
    "        l1 = l1.replace(\" (2011 Census)\",'').replace(\" (2011)\",'').replace(\"Household \",'').replace(\"House Prices\",'').replace(\"Car or van availability\",'Vehicles').replace(' (2011/12)','')\n",
    "        l1 = l1.replace('Age Structure','Age').replace(\"Ethnic Group\",'').replace('Dwelling type','').replace('Income Estimates','')\n",
    "        clabel += l1\n",
    "    if not l2.startswith(\"Unnamed\"):\n",
    "        l2 = l2.replace(\"Numbers\",'').replace(\" House Price (£)\",'').replace(\"Highest level of qualification: \",'').replace(\"Annual Household Income (£)\",'hh Income').replace('Whole house or bungalow: ','').replace(' qualifications','')\n",
    "        l2 = l2.replace('At least one person aged 16 and over in household has English as a main language',\"1+ English as a main language\").replace(\"No people in household have English as a main language\",\"None have English as main language\")\n",
    "        clabel += (' - ' if clabel != '' else '') + l2\n",
    "    if not l3.startswith(\"Unnamed\"):\n",
    "        clabel += (' - ' if clabel != '' else '') + l3\n",
    "    \n",
    "    # Replace other commonly-occuring verbiage that inflates column name width\n",
    "    clabel = clabel.replace(' -  - ',' - ').replace(\" household\",' hh').replace('Owned: ','')\n",
    "    \n",
    "    #clabel = clabel.replace(' (2011 Census)','').replace(' (2011)','').replace('Sales - 2011.1','Sales - 2012')\n",
    "    #clabel = clabel.replace('Numbers - ','').replace(' (£)','').replace('Car or van availability','Vehicles')\n",
    "    #clabel = clabel.replace('Household Income Estimates (2011/12) - ','').replace('Age Structure','Age')\n",
    "    \n",
    "    new_cols.append(clabel)\n",
    "\n",
    "print(new_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MSOA Code</th>\n",
       "      <th>MSOA Name</th>\n",
       "      <th>Age - All Ages</th>\n",
       "      <th>Age - 0-15</th>\n",
       "      <th>Age - 16-29</th>\n",
       "      <th>Age - 30-44</th>\n",
       "      <th>Age - 45-64</th>\n",
       "      <th>Age - 65+</th>\n",
       "      <th>Age - Working-age</th>\n",
       "      <th>Households - All Households</th>\n",
       "      <th>...</th>\n",
       "      <th>Adults in Employment - No adults in employment in hh: With dependent children</th>\n",
       "      <th>Total Mean hh Income</th>\n",
       "      <th>Total Median hh Income</th>\n",
       "      <th>Vehicles - No cars or vans in hh</th>\n",
       "      <th>Vehicles - 1 car or van in hh</th>\n",
       "      <th>Vehicles - 2 cars or vans in hh</th>\n",
       "      <th>Vehicles - 3 cars or vans in hh</th>\n",
       "      <th>Vehicles - 4 or more cars or vans in hh</th>\n",
       "      <th>Vehicles - Sum of all cars or vans in the area</th>\n",
       "      <th>Vehicles - Cars per hh</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>E02000001</td>\n",
       "      <td>City of London 001</td>\n",
       "      <td>7375.0</td>\n",
       "      <td>620.0</td>\n",
       "      <td>1665.0</td>\n",
       "      <td>2045.0</td>\n",
       "      <td>2010.0</td>\n",
       "      <td>1035.0</td>\n",
       "      <td>5720.0</td>\n",
       "      <td>4385.0</td>\n",
       "      <td>...</td>\n",
       "      <td>38.0</td>\n",
       "      <td>59728.481886</td>\n",
       "      <td>46788.295472</td>\n",
       "      <td>3043.0</td>\n",
       "      <td>1100.0</td>\n",
       "      <td>173.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>1692.0</td>\n",
       "      <td>0.385861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>E02000002</td>\n",
       "      <td>Barking and Dagenham 001</td>\n",
       "      <td>6775.0</td>\n",
       "      <td>1751.0</td>\n",
       "      <td>1277.0</td>\n",
       "      <td>1388.0</td>\n",
       "      <td>1258.0</td>\n",
       "      <td>1101.0</td>\n",
       "      <td>3923.0</td>\n",
       "      <td>2713.0</td>\n",
       "      <td>...</td>\n",
       "      <td>319.0</td>\n",
       "      <td>31788.185996</td>\n",
       "      <td>27058.703760</td>\n",
       "      <td>1020.0</td>\n",
       "      <td>1186.0</td>\n",
       "      <td>424.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>2305.0</td>\n",
       "      <td>0.849613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>E02000003</td>\n",
       "      <td>Barking and Dagenham 002</td>\n",
       "      <td>10045.0</td>\n",
       "      <td>2247.0</td>\n",
       "      <td>1959.0</td>\n",
       "      <td>2300.0</td>\n",
       "      <td>2259.0</td>\n",
       "      <td>1280.0</td>\n",
       "      <td>6518.0</td>\n",
       "      <td>3834.0</td>\n",
       "      <td>...</td>\n",
       "      <td>268.0</td>\n",
       "      <td>43356.931547</td>\n",
       "      <td>36834.528738</td>\n",
       "      <td>1196.0</td>\n",
       "      <td>1753.0</td>\n",
       "      <td>691.0</td>\n",
       "      <td>155.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>3766.0</td>\n",
       "      <td>0.982264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>E02000004</td>\n",
       "      <td>Barking and Dagenham 003</td>\n",
       "      <td>6182.0</td>\n",
       "      <td>1196.0</td>\n",
       "      <td>1277.0</td>\n",
       "      <td>1154.0</td>\n",
       "      <td>1543.0</td>\n",
       "      <td>1012.0</td>\n",
       "      <td>3974.0</td>\n",
       "      <td>2318.0</td>\n",
       "      <td>...</td>\n",
       "      <td>122.0</td>\n",
       "      <td>46701.436554</td>\n",
       "      <td>39668.206433</td>\n",
       "      <td>556.0</td>\n",
       "      <td>1085.0</td>\n",
       "      <td>515.0</td>\n",
       "      <td>128.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>2650.0</td>\n",
       "      <td>1.143227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>E02000005</td>\n",
       "      <td>Barking and Dagenham 004</td>\n",
       "      <td>8562.0</td>\n",
       "      <td>2200.0</td>\n",
       "      <td>1592.0</td>\n",
       "      <td>1995.0</td>\n",
       "      <td>1829.0</td>\n",
       "      <td>946.0</td>\n",
       "      <td>5416.0</td>\n",
       "      <td>3183.0</td>\n",
       "      <td>...</td>\n",
       "      <td>307.0</td>\n",
       "      <td>34293.820288</td>\n",
       "      <td>29155.683536</td>\n",
       "      <td>1080.0</td>\n",
       "      <td>1423.0</td>\n",
       "      <td>551.0</td>\n",
       "      <td>109.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>2937.0</td>\n",
       "      <td>0.922714</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 76 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   MSOA Code                 MSOA Name  Age - All Ages  Age - 0-15  \\\n",
       "0  E02000001        City of London 001          7375.0       620.0   \n",
       "1  E02000002  Barking and Dagenham 001          6775.0      1751.0   \n",
       "2  E02000003  Barking and Dagenham 002         10045.0      2247.0   \n",
       "3  E02000004  Barking and Dagenham 003          6182.0      1196.0   \n",
       "4  E02000005  Barking and Dagenham 004          8562.0      2200.0   \n",
       "\n",
       "   Age - 16-29  Age - 30-44  Age - 45-64  Age - 65+  Age - Working-age  \\\n",
       "0       1665.0       2045.0       2010.0     1035.0             5720.0   \n",
       "1       1277.0       1388.0       1258.0     1101.0             3923.0   \n",
       "2       1959.0       2300.0       2259.0     1280.0             6518.0   \n",
       "3       1277.0       1154.0       1543.0     1012.0             3974.0   \n",
       "4       1592.0       1995.0       1829.0      946.0             5416.0   \n",
       "\n",
       "   Households - All Households  ...  \\\n",
       "0                       4385.0  ...   \n",
       "1                       2713.0  ...   \n",
       "2                       3834.0  ...   \n",
       "3                       2318.0  ...   \n",
       "4                       3183.0  ...   \n",
       "\n",
       "   Adults in Employment - No adults in employment in hh: With dependent children  \\\n",
       "0                                               38.0                               \n",
       "1                                              319.0                               \n",
       "2                                              268.0                               \n",
       "3                                              122.0                               \n",
       "4                                              307.0                               \n",
       "\n",
       "   Total Mean hh Income  Total Median hh Income  \\\n",
       "0          59728.481886            46788.295472   \n",
       "1          31788.185996            27058.703760   \n",
       "2          43356.931547            36834.528738   \n",
       "3          46701.436554            39668.206433   \n",
       "4          34293.820288            29155.683536   \n",
       "\n",
       "   Vehicles - No cars or vans in hh  Vehicles - 1 car or van in hh  \\\n",
       "0                            3043.0                         1100.0   \n",
       "1                            1020.0                         1186.0   \n",
       "2                            1196.0                         1753.0   \n",
       "3                             556.0                         1085.0   \n",
       "4                            1080.0                         1423.0   \n",
       "\n",
       "   Vehicles - 2 cars or vans in hh  Vehicles - 3 cars or vans in hh  \\\n",
       "0                            173.0                             51.0   \n",
       "1                            424.0                             66.0   \n",
       "2                            691.0                            155.0   \n",
       "3                            515.0                            128.0   \n",
       "4                            551.0                            109.0   \n",
       "\n",
       "   Vehicles - 4 or more cars or vans in hh  \\\n",
       "0                                     18.0   \n",
       "1                                     17.0   \n",
       "2                                     39.0   \n",
       "3                                     34.0   \n",
       "4                                     20.0   \n",
       "\n",
       "   Vehicles - Sum of all cars or vans in the area  Vehicles - Cars per hh  \n",
       "0                                          1692.0                0.385861  \n",
       "1                                          2305.0                0.849613  \n",
       "2                                          3766.0                0.982264  \n",
       "3                                          2650.0                1.143227  \n",
       "4                                          2937.0                0.922714  \n",
       "\n",
       "[5 rows x 76 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msoa_atlas.columns = new_cols\n",
    "msoa_atlas.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 1.4: Add Inner/Outer London Mapping\n",
    "\n",
    "Using the mapping that you created last week, now try to applying it as a **lambda** function to populate a new column called `Subregion` using the `Borough` column as a source. The format for a lambda function is usually `lambda x: <code that does something with x and returns a value>`. Hint: you've got a dictionary and you know how to use it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "msoa_atlas['Borough'] = msoa_atlas['MSOA Name'].str.replace(' \\d+$','')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You might want to have a look at _what_ this drops first\n",
    "msoa_atlas.drop(index=msoa_atlas[msoa_atlas['MSOA Code'].isna()].index, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = {}\n",
    "for b in ['Enfield','Waltham Forest','Redbridge','Barking and Dagenham','Havering','Greenwich','Bexley']:\n",
    "    mapping[b]='Outer East and North East'\n",
    "for b in ['Haringey','Islington','Hackney','Tower Hamlets','Newham','Lambeth','Southwark','Lewisham']:\n",
    "    mapping[b]='Inner East'\n",
    "for b in ['Bromley','Croydon','Sutton','Merton','Kingston upon Thames']:\n",
    "    mapping[b]='Outer South'\n",
    "for b in ['Wandsworth','Kensington and Chelsea','Hammersmith and Fulham','Westminster','Camden','City of London']:\n",
    "    mapping[b]='Inner West'\n",
    "for b in ['Richmond upon Thames','Hounslow','Ealing','Hillingdon','Brent','Harrow','Barnet']:\n",
    "    mapping[b]='Outer West and North West'\n",
    "\n",
    "msoa_atlas['Subregion'] = msoa_atlas.Borough.apply(lambda x: mapping[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/sophieayling/Documents/GitHub/i2p/practicals\n"
     ]
    }
   ],
   "source": [
    "cd '/Users/sophieayling/Documents/GitHub/i2p/practicals/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### And Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "msoa_atlas.to_csv(os.path.join('..','data','raw','MSOA_Atlas.csv.gz'), index=False)\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 1.5: Merge with MSOA Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "DriverError",
     "evalue": "data/geo/London_MSOAs.gpkg: No such file or directory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCPLE_OpenFailedError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32mfiona/_shim.pyx\u001b[0m in \u001b[0;36mfiona._shim.gdal_open_vector\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mfiona/_err.pyx\u001b[0m in \u001b[0;36mfiona._err.exc_wrap_pointer\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mCPLE_OpenFailedError\u001b[0m: data/geo/London_MSOAs.gpkg: No such file or directory",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mDriverError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-a1d4fe5d3503>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmsoas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'geo'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'London_MSOAs.gpkg'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdriver\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'GPKG'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda3/envs/i2p/lib/python3.7/site-packages/geopandas/io/file.py\u001b[0m in \u001b[0;36m_read_file\u001b[0;34m(filename, bbox, mask, rows, **kwargs)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mfiona_env\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mreader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_bytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0;31m# In a future Fiona release the crs attribute of features will\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/i2p/lib/python3.7/site-packages/fiona/env.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    396\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlocal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_env\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 398\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    399\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/i2p/lib/python3.7/site-packages/fiona/__init__.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode, driver, schema, crs, encoding, layer, vfs, enabled_drivers, crs_wkt, **kwargs)\u001b[0m\n\u001b[1;32m    252\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'a'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m             c = Collection(path, mode, driver=driver, encoding=encoding,\n\u001b[0;32m--> 254\u001b[0;31m                            layer=layer, enabled_drivers=enabled_drivers, **kwargs)\n\u001b[0m\u001b[1;32m    255\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/i2p/lib/python3.7/site-packages/fiona/collection.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path, mode, driver, schema, crs, encoding, layer, vsi, archive, enabled_drivers, crs_wkt, ignore_fields, ignore_geometry, **kwargs)\u001b[0m\n\u001b[1;32m    152\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'a'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWritingSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mfiona/ogrext.pyx\u001b[0m in \u001b[0;36mfiona.ogrext.Session.start\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mfiona/_shim.pyx\u001b[0m in \u001b[0;36mfiona._shim.gdal_open_vector\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mDriverError\u001b[0m: data/geo/London_MSOAs.gpkg: No such file or directory"
     ]
    }
   ],
   "source": [
    "msoas = gpd.read_file(os.path.join('data','geo','London_MSOAs.gpkg'), driver='GPKG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = pd.merge(msoas, msoa_atlas, left_on='MSOA11CD', right_on='MSOA Code', how='inner')\n",
    "\n",
    "gdf = gdf.drop(columns=['MSOA11CD','MSOA11NM', 'Borough_x']).rename(columns={'Borough_y':'Borough'})\n",
    "\n",
    "print(f\"Final MSOA Atlas data frame has shape {gdf.shape[0]:,} x {gdf.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should get `Final data frame has shape 983 x 83`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf.plot(column='Median - 2011', cmap='plasma', \n",
    "         scheme='FisherJenks', k=7, edgecolor='None', legend=True, figsize=(9,7));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save as GeoPackage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf.to_file(os.path.join('data','geo','MSOA_Atlas.gpkg'), driver='GPKG')\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Splitting a Data Set into Test & Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A standard approach to Machine Learning, and something that is becoming more widely used elsewhere, is the splitting of a large data into set into testing and training components. Typically, you would take 80-90% of your data to 'train' your algorithm and withold between 10-20% for validation ('testing'). An even 'stricter' approach, in the sense of trying to ensure the robustness of your model against outlier effects, is [cross validation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_validate.html) such as [k-folds cross-validation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html). Here we are just going to use it to explore the issues raised in normalisation and standardisation by the use of SciKit-Learn's pipeline architecture with streaming data.\n",
    "\n",
    "So Sci-Kit Learn is often used in a model-validation context in which we are trying to _predict_ something. So Sci-Kit Learn _expects_ that you'll have an `X` which is your **predictors** (the inputs to your model) and a `y` which is the thing you're **trying to predict**. We're obviously not building a model here (that's for Term 2!) so we'll just 'pretend' that we're trying to predict the price of a listing and will set that up as our `y` data set. Notice too that you can pass a data frame directly to Sci-Kit Learn and it will split it for you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reload\n",
    "\n",
    "On subsequent runs of this notebook you might just want to start here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = gpd.read_file(os.path.join('data','geo','MSOA_Atlas.gpkg'), driver='GPKG')\n",
    "print(gdf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categoricals = ['Borough','Subregion']\n",
    "for c in categoricals:\n",
    "    gdf[c] = gdf[c].astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our purposes this is a little bit overkill as you could also use pandas' `sample(frac=0.2)` and the indexes, but it's useful to see how this works. Ordinarily, you would use this when building a model where you have both predictors _and_ a target. These would be held separately and then you use test/train split to ensure that you get four data sets out the training (predictors + target as separate data sets) and the training (predictors + target as separate data sets) accoridng to the `test_size` specfied in the `test_train_split` parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "pdf = gdf['Median - 2011'].copy() # pdf for Median *P*rice b/c we need *something*\n",
    "\n",
    "df_train, df_test, pr_train, pr_test = train_test_split(gdf, pdf, test_size=0.2, random_state=44)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below you should see that the data has been split roughly on the basis of the `test_size` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Original data size: {gdf.shape[0]:,} x {gdf.shape[1]}\")\n",
    "print(f\"Training data size: {df_train.shape[0]:,} x {df_train.shape[1]}\")\n",
    "print(f\"Testing data size:    {df_test.shape[0]:,} x {df_test.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also notice the indexes of the testing data sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\", \".join([str(x) for x in df_train.index[:10]]))\n",
    "print(\", \".join([str(x) for x in pr_train.index[:10]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 2.1: Plotting the Test/Train Data Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f,axes = plt.subplots(1,2, figsize=(12,5))\n",
    "df_train.plot(ax=axes[0])\n",
    "df_test.plot(ax=axes[1])\n",
    "axes[0].set_title('Training Data')\n",
    "axes[1].set_title('Testing Data');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Normalisation\n",
    "\n",
    "The developers of [SciKit-Learn](https://scikit-learn.org/) define [normalisation](https://scikit-learn.org/stable/modules/preprocessing.html#normalization) as \"scaling individual samples to have **unit norm**.\" There are a _lot_ of subtleties to this when you start dealing with 'sparse' data, but for the most part it's worthwhile to think of this as a rescaling of the raw data to have similar ranges in order achieve some kind of comparison. This is such a common problem that sklearn offers a range of such (re)scalers including: `MinMaxScaler`.\n",
    "\n",
    "Let's see what effect this has on the data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sets some handy 'keywords' to tweak the Seaborn plot\n",
    "kwds = dict(s=7,alpha=0.95,edgecolor=\"none\")\n",
    "# Set the *hue order* so that all plots have some colouring by Subregion\n",
    "ho = ['Inner East','Inner West','Outer West and North West','Outer South','Outer East and North East']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 3.1: Range Normalisation\n",
    "\n",
    "There is some 'dense' code in here, make sure you that you understand what is happening in the loops and the dataframe copies! One thing you'll need to explain is why I keep writing `df[cols+['Subregion']` and why I don't just add it to the `cols` variable at the start?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['Tenure - Owned outright', 'Tenure - Owned with a mortgage or loan',\n",
    "        'Tenure - Social rented', 'Tenure - Private rented']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Notice what this is doing! See if you can explain it clearly.\n",
    "scalers = [MinMaxScaler().fit(df_train[x].values.reshape(-1,1)) for x in cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_normed = df_train[cols+['Subregion']].copy()\n",
    "\n",
    "for i in range(0, len(cols)):\n",
    "    # Ditto this -- can you explain what this code is doing\n",
    "    tr_normed[cols[i]] = scalers[i].transform(df_train[cols[i]].values.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst_normed = df_test[cols+['Subregion']].copy()\n",
    "\n",
    "for i in range(0, len(cols)):\n",
    "    # What is this doing differently?\n",
    "    tst_normed[cols[i]] = scalers[i].transform(df_test[cols[i]].values.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_normed.columns  = [re.sub('( - |/)',\"\\n\",x) for x in tr_normed.columns.values]\n",
    "tst_normed.columns = [re.sub('( - |/)',\"\\n\",x) for x in tst_normed.columns.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(data=df_train[cols+['Subregion']], hue='Subregion', diag_kind='kde', corner=True, plot_kws=kwds, hue_order=ho);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(data=tr_normed, hue='Subregion', diag_kind='kde', corner=True, plot_kws=kwds, hue_order=ho);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(data=tst_normed, hue='Subregion', diag_kind='kde', corner=True, plot_kws=kwds, hue_order=ho);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 3.2: Why???\n",
    "\n",
    "Why do I keep writing `df[cols+['Subregion']`? Why I don't just add it to the `cols` variable at the start?\n",
    "\n",
    "> Because then it would be included in our transformation code! It's easier, for me at least, to add in the Subregions where I need them than to have to constantly remember to skip the last element in the `cols` array!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 3.3: Change, change, changes!\n",
    "\n",
    "What do you notice about the `df_train` (raw training data) and `tr_normed` (normalised training data) data sets? What do you notice about the `tst_normed` results? What is the _potential_ problem that different 'realisations' of `tst_normed` might cause.\n",
    "\n",
    "> The normlised testing data _can_ have values that exceed 1. So this might cause issues where you _expect_ your MinMaxScaler to always produce results in the range 0..1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Standardisation \n",
    "\n",
    "Recall that standardisation is typically focussed on rescaling data to have a mean (or median) of 0 and standard deviation or IQR of 1 and that these approaches are therefore closely tied to the idea of the standard normal distribution. However, and rather confusingly, many data scientists will refer to standardisation and normalisation largely interchangeably."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import RobustScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = 'Vehicles - No cars or vans in hh'\n",
    "tr  = df_train[[col]].copy()\n",
    "tst = df_test[[col]].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 4.1: Z-Score Standardisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = StandardScaler()\n",
    "\n",
    "ss.fit(tr[col].values.reshape(-1,1))\n",
    "\n",
    "tr[f\"Z. {col}\"]  = ss.transform(tr[col].values.reshape(-1,1))\n",
    "tst[f\"Z. {col}\"] = ss.transform(tst[col].values.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 4.2: Inter-Quartile Standardisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs = RobustScaler(quantile_range=(25.0, 75.0))\n",
    "\n",
    "rs.fit(tr[col].values.reshape(-1,1))\n",
    "\n",
    "tr[f\"IQR. {col}\"] = rs.transform(tr[col].values.reshape(-1,1))\n",
    "tst[f\"IQR. {col}\"] = rs.transform(tst[col].values.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.jointplot(data=tr, x=f\"Z. {col}\", y=f\"IQR. {col}\", kind='hex'); # hex probably not the best choice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perhaps a little more useful..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.kdeplot(tr[f\"Z. {col}\"])\n",
    "sns.kdeplot(tr[f\"IQR. {col}\"], color='r', ax=ax)\n",
    "plt.legend(loc='upper right', labels=['Standard', 'Robust']) # title='Foo'\n",
    "ax.ticklabel_format(useOffset=False, style='plain')\n",
    "ax.set_xlabel(\"Standardised Value for No cars or vans in hh\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 4.3: Differnces?\n",
    "\n",
    "Can you see the differences between these two rescalers, and can you explain why you might want to choose one over the other?\n",
    "\n",
    "> Notice that the Robust Scaler output now has less skew than the output from the Standard Scaler. So when dealing with heavy skew you'd find the Robust scaler more useful. If you treated the skewed data using z-scores you'd be more likely to find values _statistically significant_ on either side. This problem also crops up in spatial statistics for quite similar reasons. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Non-Linear Transformations\n",
    "\n",
    "So transformations are useful when a data series has features that make comparisons or analysis difficult, or that affect our ability to intuit meaningful difference. By manipulating the data using one or more mathematical operations we can sometimes make it more *tractable* for subsequent analysis. In other words, it's all about the _context_ of our data.\n",
    "\n",
    "[![How tall is tall?](http://img.youtube.com/vi/-VjcCr4uM6w/0.jpg)](http://www.youtube.com/watch?v=-VjcCr4uM6w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From above, we know the _MedianIncome_ data are _not_ normally distributed, but can we work out what distribution best represents _MedianIncome_? This can be done by comparing the shape of the histogram to the shapes of theoretical distributitions. For example:\n",
    "\n",
    "- the [log-normal](https://en.wikipedia.org/wiki/Log-normal_distribution) distribution\n",
    "- the [exponential](https://en.wikipedia.org/wiki/Exponential_distribution) distribution\n",
    "- the [Poisson](https://en.wikipedia.org/wiki/Poisson_distribution) distribution (for non-continuous data)\n",
    " \n",
    "From looking at those theoretical distributions, we might make an initial guess as to the type of distribution. There are actually _many_ other distributions encountered in real life data, but these ones are particuarly common. A wider view of this would be that [quantile and power transformations](https://scikit-learn.org/stable/modules/preprocessing.html#non-linear-transformation) are ways of preserving the rank of values but lose many of the other features of the relationships that might be preserved by, for instance, the standard scaler.\n",
    "\n",
    "In the case of Median Income, taking a log-transform of the data might make it _appear_ more normal: you do **not** say that a transformation _makes_ data more normal, you say either that 'it allows us to treat the data as normally distributed' or that 'the transformed data follows a log-normal distribution'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 5.1: The Normal Distribution\n",
    "\n",
    "Z-scores are often associated with the normal distribution because their interpretation implicitly assumes a normal distribution. Or to put it another way... You can always calculate z-scores for your data (it's just a formula applied to data points), but their _intuitive meaning_ is lost if your data don't have something like a normal distribution (or follow the [68-95-99.7 rule](https://en.wikipedia.org/wiki/68–95–99.7_rule)).\n",
    "\n",
    "But... what if our data are non-normal? Well, Just because data are non-normal doesn't mean z-scores can't be calculated; we just have to be careful what we do with them... and sometimes we should just avoid them entirely. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a function to create that theoretical normal distribution. See if you can understand what's going and add comments to the code to explain what each line does. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normal_from_dist(series):  #define function name and required arguments (in this case a pandas series)\n",
    "    mu = series.mean()         #calculate the mean of our data\n",
    "    sd = series.std()          #calculate the standard deviation of our data\n",
    "    n  = len(series)           #count how many observations are in our data\n",
    "    s = np.random.normal(mu, sd, n)   #use the parameters of the data just calculated to generate n random numbers, drawn from a normal distributions \n",
    "    return s                   #return this set of random numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make it easier to understand what the function above is doing, let's use it! We'll use the function to plot both a distribution plot with both histogram and KDE for our data, and then add a _second_ overplot distplot to the same fig showing the theoretical normal distribution (in red). We'll do this in a loop for each of the three variables we want to examine.\n",
    "\n",
    "**From the output, which of the variables has a roughly normal distribution?** Another way to think about this question is, for which of the variables are the mean and standard deviation _most_ appropriate as measures of centrality and spread?\n",
    "\n",
    "**Also**, how would you determine the _meaning_ of some of the departures from the normal distribution?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selection = [x for x in df_train.columns.values if x.startswith('Composition')]\n",
    "\n",
    "for c in selection:\n",
    "    ax = sns.kdeplot(df_train[c])\n",
    "    sns.kdeplot(normal_from_dist(df_train[c]), color='r', fill=True, ax=ax)\n",
    "    plt.legend(loc='upper right', labels=['Observed', 'Normal']) # title='Foo'\n",
    "    ax.ticklabel_format(useOffset=False, style='plain')\n",
    "    if ax.get_xlim()[1] > 999999:\n",
    "        plt.xticks(rotation=45)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 5.2: Am I Normal?\n",
    "\n",
    "Which, if any, of the variables has a roughly normal distribution? Another way to think about this question is, for which of the variables are the mean and standard deviation _most_ appropriate as measures of centrality and spread?\n",
    "\n",
    "> The couple distributions are quite close to normal when compared to other household compositions. Although these results haven't been area-standardised (i.e. by taking the LQ or share) there's certainly ana interesting suggestion that there are some areas where there are a _lot_ of one person households!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 5.3: Meaningful? \n",
    "\n",
    "How might you determine the _significance_ of some of the departures from the normal distribution?\n",
    "\n",
    "> I've largely answered this above: some particularly high values are quite interesting and would almost certainly warrant additional investigation if you were so minded. However, do keep in mind that we're looking at raw counts here and not the standardised values that would take into accout the size of the MSOA! I didn't want to add too much to each practical, but using the LQ or proportion/share here would be a very good idea."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 5.4: Logarithmic Transformations\n",
    "\n",
    "To create a new series in the data frame containing the natural log of the original value it’s a similar process to what we've done before, but since pandas doesn't provide a log-transform operator (i.e. you can’t call `df['MedianIncome'].log()` ) we need to use the `numpy` package since pandas data series are just numpy arrays with some fancy window dressing that makes them even _more_ useful:\n",
    "```python\n",
    "series = pd.Series(np.log(df.<series>))\n",
    "```\n",
    "Let's perform the transform then compare to the un-transformed data. Comment the code below to ensure that you understand what it is doing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['Median - 2011','Total Mean hh Income']\n",
    "\n",
    "for m in cols:\n",
    "    s  = df_train[m] # s == series\n",
    "    ts = np.log(s)   # ts == transformed series\n",
    "    \n",
    "    ax = sns.kdeplot(s)\n",
    "    sns.kdeplot(normal_from_dist(s), color='r', fill=True, ax=ax)\n",
    "    plt.legend(loc='upper right', labels=['Observed', 'Normal']) # title also an option\n",
    "    \n",
    "    ### USEFUL FORMATTING TRICKS ###\n",
    "    # This turns off scientific notation in the ticklabels\n",
    "    ax.ticklabel_format(useOffset=False, style='plain')\n",
    "    # Notice this snippet of code\n",
    "    ax.set_xlabel(ax.get_xlabel() + \" (Raw Distribution)\")\n",
    "    # Notice this little code snippet too\n",
    "    if ax.get_xlim()[1] > 999999:\n",
    "        plt.xticks(rotation=45)\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    ax = sns.kdeplot(ts)\n",
    "    sns.kdeplot(normal_from_dist(ts), color='r', fill=True, ax=ax)\n",
    "    plt.legend(loc='upper right', labels=['Observed', 'Normal'])\n",
    "    ax.ticklabel_format(useOffset=False, style='plain')\n",
    "    ax.set_xlabel(ax.get_xlabel() + \" (Logged Distribution)\")\n",
    "    if ax.get_xlim()[1] > 999999:\n",
    "        plt.xticks(rotation=45)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopefully, you can see that the transformed data do indeed look 'more normal'; the peak of the red and blue lines are closer together and the blue line at the lower extreme is also closer to the red line, but we can check this by seeing what has happened to the z-scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Principal Components Analysis\n",
    "\n",
    "Now we're going to ask the question: how can we represent our data using a smaller number of components that capture the variance in the original data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optional Reload\n",
    "\n",
    "Use this is your data gets messy..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = gpd.read_file(os.path.join('data','geo','MSOA_Atlas.gpkg'), driver='GPKG')\n",
    "print(gdf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categoricals = ['Borough','Subregion']\n",
    "for c in categoricals:\n",
    "    gdf[c] = gdf[c].astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 6.1: Removing Non-Numeric Data\n",
    "\n",
    "To perform PCA we can only have numeric data. In theory, categorical data can be converted to numeric and retained, but there are two issues:\n",
    "\n",
    "1. Nominal data has no _innate_ order so we _can't_ convert > 2 categories to numbers and have to convert them to One-Hot Encoded values.\n",
    "2. A binary (i.e. One-Hot Encoded) variable will account for a _lot_ of variance in the data because it's only two values are 0 and 1!\n",
    "\n",
    "So in practice, it's probably a good idea to drop categorical data if you're planning to use PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_drop = ['OBJECTID', 'BNG_E', 'BNG_N', 'msoa11hclnm', \n",
    "           'MSOA Name', 'Borough', 'Subregion', 'geometry']\n",
    "gdf_pca = gdf.drop(columns=to_drop).set_index('MSOA Code').copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 6.2: Rescaling\n",
    "\n",
    "In order to ensure that our results aren't dominated by a single scale (e.g. House Prices!) we need to rescale all of our data. You could easily try different scalers as well as a different parameters to see what effect this has on your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rs = RobustScaler(quantile_range=(10.0, 90.0))\n",
    "\n",
    "for c in gdf_pca.columns.values:\n",
    "    gdf_pca[c] = rs.fit_transform(gdf_pca[c].values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 6.3: Perform PCA on Rescaled Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA \n",
    "\n",
    "pca = PCA(n_components=50, whiten=True) \n",
    "\n",
    "pca.fit(gdf_pca)\n",
    "\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "singular_values = pca.singular_values_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 6.4: Examine Explained Variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(1,len(explained_variance)+1)\n",
    "plt.plot(x, explained_variance)\n",
    "plt.ylabel('Share of Variance Explained')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, 20):\n",
    "    print(f\"Component {i:>2} accounts for {explained_variance[i]*100:>2.2f}% of variance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 6.5: How Many Components?\n",
    "\n",
    "There are a number of ways that we could set a threshold for dimensionality reduction: \n",
    "- The most common is to look for the 'knee' in the Explained Variance plot above. That would put us at about 5 retained components.\n",
    "- Another is to just keep all components contributing more than 1% of the variance. That would put us at about 10 components.\n",
    "- You can also ([I discovered](https://medium.com/@nikolay.oskolkov/hi-jon-reades-my-sincere-apologies-for-this-very-late-reply-444f57054d14)) look to shuffle the data and repeatedly perform PCA to build confidence intervals. I have not implemented this (yet).\n",
    "\n",
    "In order to _do_ anything with these components we need to somehow reattach them to the MSOAs. So that entails taking the transformed results (`X_train` and `X_test`) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_n_components = 10\n",
    "\n",
    "# If we weren't changing the number of components we\n",
    "# could re-use the pca object created above. \n",
    "pca = PCA(n_components=keep_n_components, whiten=True)\n",
    "\n",
    "X_train = pca.fit_transform(gdf_pca)\n",
    "\n",
    "# Notice that we get the _same_ values out,\n",
    "# so this is a *deterministic* process that\n",
    "# is fully replicable (allowing for algorithmic\n",
    "# and programming language differences).\n",
    "for i in range(0, keep_n_components):\n",
    "    print(f\"Component {i:>2} accounts for {pca.explained_variance_ratio_[i]*100:>2.2f}% of variance\")\n",
    "\n",
    "# Notice...\n",
    "print(len(X_train))\n",
    "print(gdf_pca.shape[0])\n",
    "# So each observation has a row in X_train and there is \n",
    "# 1 column for each component. This defines the mapping\n",
    "# of the original data space into the reduced one\n",
    "print(len(X_train[0])) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 6.6: Components to Columns\n",
    "\n",
    "You could actually do this more quickly (but less clearly) using `X_train.T` to transpose the matrix and then you'd have one row per component instead of one column!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in [X_train]:\n",
    "    new_columns = []\n",
    "    \n",
    "    for i in range(0,keep_n_components):\n",
    "        new_columns.append([])\n",
    "\n",
    "    for i in x:\n",
    "        for j in range(0,keep_n_components):\n",
    "            new_columns[j].append(i[j])\n",
    "\n",
    "    for i in range(0,keep_n_components):\n",
    "        gdf_pca[f\"Component {i+1}\"] = new_columns[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_pca.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 6.7: Attaching GeoData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msoas = gpd.read_file(os.path.join('data','geo','London_MSOAs.gpkg'), driver='GPKG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcadf = pd.merge(msoas.set_index('MSOA11CD'), gdf_pca, left_index=True, right_index=True, how='inner')\n",
    "print(f\"PCA df has shape {pcadf.shape[0]} x {pcadf.shape[1]}\")\n",
    "pcadf.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should get `Final data frame has shape 983 x 91`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 6.8: Map the First _n_ Components\n",
    "\n",
    "How would you automate this so that the loop creates one plot for each component?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,4):\n",
    "    ax = pcadf.plot(column=f'Component {i}', cmap='plasma', \n",
    "         scheme='FisherJenks', k=7, edgecolor='None', legend=True, figsize=(9,7));\n",
    "    ax.set_title(f'PCA Component {i}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. t-SNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 7.1: t-SNE Using PCA Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pcadf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-2ce230f3b883>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mprp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mtsnedf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpcadf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Component 1'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34mf\"Component {keep_n_components}\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mtsne\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTSNE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeep_dims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mperplexity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pcadf' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# You might want to experiment with all\n",
    "# 3 of these values -- it may make sense \n",
    "# to package a lot of this up into a function!\n",
    "keep_dims=2\n",
    "lrn_rate=150\n",
    "prp=50\n",
    "\n",
    "tsnedf = pcadf.loc[:,'Component 1':f\"Component {keep_n_components}\"].copy()\n",
    "\n",
    "tsne = TSNE(n_components=keep_dims, perplexity=prp, random_state=42, n_iter=5000, n_jobs=-1)\n",
    "X_embedded = tsne.fit_transform(tsnedf)\n",
    "X_embedded.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 7.2: Copy Dimensions Back to Data Frame\n",
    "\n",
    "Can probably also be solved using `X_embedded.T`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in [X_embedded]:\n",
    "    new_columns = []\n",
    "    \n",
    "    for i in range(0,keep_dims):\n",
    "        new_columns.append([])\n",
    "\n",
    "    for i in x:\n",
    "        for j in range(0,keep_dims):\n",
    "            new_columns[j].append(i[j])\n",
    "\n",
    "    for i in range(0,keep_dims):\n",
    "        tsnedf[f\"Dimension {i+1}\"] = new_columns[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 7.3: Merge and Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rddf = pd.merge(msoas.set_index('MSOA11CD'), tsnedf, left_index=True, right_index=True, how='inner')\n",
    "rddf.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 7.4: Visualise!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rddf['Subregion'] = rddf.Borough.apply(lambda x: mapping[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sets some handy 'keywords' to tweak the Seaborn plot\n",
    "kwds = dict(s=7,alpha=0.95,edgecolor=\"none\")\n",
    "# Set the *hue order* so that all plots have some colouring by Subregion\n",
    "ho = ['Inner East','Inner West','Outer West and North West','Outer South','Outer East and North East']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.jointplot(data=rddf, x='Dimension 1', y='Dimension 2', \n",
    "                  hue='Subregion', hue_order=ho, joint_kws=kwds)\n",
    "g.ax_joint.legend(loc='upper right', prop={'size': 8});"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for r in rddf.Subregion.unique():\n",
    "    g = sns.jointplot(data=rddf[rddf.Subregion==r], x='Dimension 1', y='Dimension 2', \n",
    "                  hue='Borough', joint_kws=kwds)\n",
    "    g.ax_joint.legend(loc='upper right', prop={'size': 8});\n",
    "    plt.suptitle(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can't unfortunately do any clustering at this point to create groups from the data (that's next week!) so for now note that there are several large-ish groups (in terms of membership) and few small ones picked up by t-SNE. Alos note that there is strong evidence of some incipient structure: Inner East and West largely clump together, while Outher East and Outer South also seem to group together, with Outer West being more distinctive. If you look back at the PCA Components (especially \\#1) you might be able to speculate about some reasons for this! Please note: this is _only_ speculation at this time!\n",
    "\n",
    "Next week we'll also add the listings data back in as part of the picture!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### And Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rddf.to_file(os.path.join('data','clean','Reduced_Dimension_Data.gpkg'), driver='GPKG')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Credits!\n",
    "\n",
    "#### Contributors:\n",
    "The following individuals have contributed to these teaching materials: Jon Reades (j.reades@ucl.ac.uk).\n",
    "\n",
    "#### License\n",
    "These teaching materials are licensed under a mix of [The MIT License](https://opensource.org/licenses/mit-license.php) and the [Creative Commons Attribution-NonCommercial-ShareAlike 4.0 license](https://creativecommons.org/licenses/by-nc-sa/4.0/).\n",
    "\n",
    "#### Potential Dependencies:\n",
    "This notebook may depend on the following libraries: pandas, geopandas, sklearn, matplotlib, seaborn"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
